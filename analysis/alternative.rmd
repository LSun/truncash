---
title: "True Signal vs Correlated Null"
author: "Lei Sun"
date: 2017-03-29
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

## Identifiablity of true signals from correlated noise

[We've shown](gaussian_derivatives_2.html#examples) that in many real data sets when we have correlated null $z$ scores, we can [fit their empirical distribution with Gaussian and its derivatives](gaussian_derivatives.html#empirical_null).

But what if we have true signals instead of the global null?  Theoretically, any distribution can be decomposed by Gaussian and its derivatives, also called [Edgeworth series or Edgeworth expansion](https://en.wikipedia.org/wiki/Edgeworth_series).  We've shown that the Dirac delta function $\delta_z$ and the associated $0$-$1$ step function [can be decomposed](gaussian_derivatives_4.html#extreme_case:_(rho_{ij}_equiv_1)) by Gaussian derivatives.  Essentially all distributions can be represented by (usually infinitely many) $\delta_z$, and thus be decomposed by Gaussian and its derivatives.  __There is a rich literature on this topic, probably of further use to this project.__

Now the more urgent problem is: can true signals also be fitted by Gaussian derivatives in a similar way as correlated null?  Let normalized weights $W_k^s = W_k\sqrt{k!}$.  As [shown previously](gaussian_derivatives.html), under correlated null, the variance $\text{var}(W_k^s) = \alpha_k = \bar{\rho_{ij}^k}$.  Thus, under correlated null, the Gaussian derivative decomposition of the empirical distribution should have "reasonable" weights of similar decaying patterns.

If it turns out Gaussian derivatives with limited orders (say, $K \leq 10$) and reasonable normalized weights are only able to fit the empirical correlated null, but nothing else, then properly regularized Gaussian derivatives can be readily used to control the usually correlated *noise*, which are correlated null, and leave the *signal* to `ash`.  But if true signals can also be fitted this way, the identifiability of true signals from correlated noise becomes an issue.

Let's start with the simplest case: $z \sim N(0, \sqrt{2}^2)$ independently.  This data set can be seen as generated as follows.

$$
\begin{array}{c}
\beta_j \sim N(0, 1)\\
z_j \sim N(\beta_j, 1)
\end{array}
$$

That is, a $N(0, 1)$ true signal is polluted by a $N(0, 1)$ noise.

## Illustration

```{r, cache = TRUE}
n = 1e4
m = 5
set.seed(777)
zmat = matrix(rnorm(n * m, 0, sd = sqrt(2)), nrow = m)
```

```{r, message = FALSE, cache = TRUE, result = "hide"}
library(ashr)
source("../code/ecdfz.R")
res = list()
for (i in 1:3) {
  z = zmat[i, ]
  p = (1 - pnorm(abs(z))) * 2
  bh.fd = sum(p.adjust(p, method = "BH") <= 0.05)
  pihat0.ash = get_pi0(ash(z, 1, method = "fdr"))
  ecdfz.fit = ecdfz.optimal(z)
  res[[i]] = list(z = z, p = p, bh.fd = bh.fd, pihat0.ash = pihat0.ash, ecdfz.fit = ecdfz.fit)
}
```
```{r, cache = TRUE, include = FALSE}
x = seq(-5, 5, 0.01)
H.x = sapply(1:8, EQL::hermite, x = x)
```

```{r, cache = TRUE, echo = FALSE}
K = c(7, 8, 8)
for (i in 1:3) {
  cat("Number of Discoveries:", res[[i]]$bh.fd, "; pihat0 =", res[[i]]$pihat0.ash, "\n")
  cat("Log-likelihood with N(0, 2):", sum(log(dnorm(res[[i]]$z, mean = 0, sd = sqrt(2)))), "\n")
  cat("Log-likelihood with Gaussian Derivatives:", -res[[i]]$ecdfz.fit$res[[K[i]]]$optimal_value + sum(log(dnorm(res[[i]]$z))), "\n")
  cat("Log-likelihood ratio between true N(0, 2) and fitted Gaussian derivatives:", sum(log(dnorm(res[[i]]$z, mean = 0, sd = sqrt(2)))) - (-res[[i]]$ecdfz.fit$res[[K[i]]]$optimal_value + sum(log(dnorm(res[[i]]$z)))), "\n")
  cat("Normalized weights:\n")
  w = res[[i]]$ecdfz.fit$res[[K[i]]]$primal_values[[1]]
  cat(rbind(paste(1:K[i], ":"), paste(w * sqrt(factorial(1:K[i])), ";")), sep = " ")
  hist(res[[i]]$z, breaks = 100, prob = TRUE, ylim = c(0, dnorm(0)), xlab = "z", main = "Histogram of z", xlim = range(x))
  lines(x, dnorm(x), col = "red")
  lines(x, dnorm(x, 0, sqrt(2)), col = "red", lty = 2)
  y = dnorm(x) * (H.x[, 1:K[i]] %*% res[[i]]$ecdfz.fit$res[[K[i]]]$primal_values[[1]] + 1)
  lines(x, y, col = "blue")
  legend("topright", col = c("red", "blue", "red"), lty = c(1, 1, 2), c("N(0, 1)", paste("K =", K[i]), "N(0, 2)"))

  tail = c(3, 5)

  cat("Zoom in to the left tails:\n")
  hist(res[[i]]$z, breaks = 100, prob = TRUE, xlab = "z", main = "Histogram of z", xlim = sort(-tail), ylim = c(0, dnorm(min(abs(tail)), 0, sqrt(2))))
  lines(x, dnorm(x), col = "red")
  lines(x, dnorm(x, 0, sqrt(2)), col = "red", lty = 2)
  y = dnorm(x) * (H.x[, 1:K[i]] %*% res[[i]]$ecdfz.fit$res[[K[i]]]$primal_values[[1]] + 1)
  lines(x, y, col = "blue")
  legend("topleft", col = c("red", "blue", "red"), lty = c(1, 1, 2), c("N(0, 1)", paste("K =", K[i]), "N(0, 2)"))
  
  cat("Zoom in to the right tails:\n")
  hist(res[[i]]$z, breaks = 100, prob = TRUE, xlab = "z", main = "Histogram of z", xlim = sort(tail), ylim = c(0, dnorm(min(abs(tail)), 0, sqrt(2))))
  lines(x, dnorm(x), col = "red")
  lines(x, dnorm(x, 0, sqrt(2)), col = "red", lty = 2)
  y = dnorm(x) * (H.x[, 1:K[i]] %*% res[[i]]$ecdfz.fit$res[[K[i]]]$primal_values[[1]] + 1)
  lines(x, y, col = "blue")
  legend("topright", col = c("red", "blue", "red"), lty = c(1, 1, 2), c("N(0, 1)", paste("K =", K[i]), "N(0, 2)"))

  qqnorm(res[[i]]$z, main = "Normal Q-Q plot for z scores")
  abline(0, 1)
  
  m = n
  pj = sort(res[[i]]$p)
  plot(pj, xlab = "Order", ylab = "Ordered p value", main = "All p values")
  abline(0, 1 / m, col = "blue")
  abline(0, 0.05 / m, col = "red")
  legend("top", lty = 1, col = c("blue", "red"), c("Uniform", "BH, FDR = 0.05"))

  plot(pj[1:max(100, res[[i]]$bh.fd)], xlab = "Order", ylab = "Ordered p value", main = "Zoom-in to 100 smallest p values", ylim = c(0, pj[max(100, res[[i]]$bh.fd)]))
  abline(0, 1 / m, col = "blue")
  points(pj[1:res[[i]]$bh.fd], col = "green", pch = 19)
  abline(0, 0.05 / m, col = "red")
  legend("top", lty = 1, col = c("blue", "red"), c("Uniform", "BH, FDR = 0.05"))
  
  plot(pj[pj <= 0.01], xlab = "Order", ylab = "Ordered p value", main = "Zoom-in to p values <= 0.01", ylim = c(0, 0.01))
  abline(0, 1 / m, col = "blue")
  points(pj[1:res[[i]]$bh.fd], col = "green", pch = 19)
  abline(0, 0.05 / m, col = "red")
  legend("top", lty = 1, col = c("blue", "red"), c("Uniform", "BH, FDR = 0.05"))
  
  plot(pj[pj <= 0.05], xlab = "Order", ylab = "Ordered p value", main = "Zoom-in to p values <= 0.05", ylim = c(0, 0.05))
  abline(0, 1 / m, col = "blue")
  points(pj[1:res[[i]]$bh.fd], col = "green", pch = 19)
  abline(0, 0.05 / m, col = "red")
  legend("top", lty = 1, col = c("blue", "red"), c("Uniform", "BH, FDR = 0.05"))
}
```

## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
