---
title: "True Signal vs Correlated Null"
author: "Lei Sun"
date: 2017-03-29
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

## Introduction

[We've shown](gaussian_derivatives_2.html) that in many real data sets when we have correlated null $z$ scores, we can fit their empirical distribution with Gaussian and its derivatives.

But what if we have real signals instead of the global null?  Can we fit real signals with Gaussian derivatives with reasonable weights?  Let's start with the most basic case: $z \sim N(0, \sqrt{2}^2)$ independently.  This data set can be seen as generated as follows.

$$
\begin{array}{c}
\beta_j \sim N(0, 1)\\
z_j \sim N(\beta_j, 1)
\end{array}
$$

## Illustration

Let normalized weights $w_k^s = w_k\sqrt{k!}$ with variance $\text{var}(w_k^s) = \alpha_k = 
\bar{\rho_{ij}^k}$.

```{r, cache = TRUE}
n = 1e4
m = 5
set.seed(777)
zmat = matrix(rnorm(n * m, 0, sd = sqrt(2)), nrow = m)
```

```{r, message = FALSE, cache = TRUE, result = "hide"}
library(ashr)
source("../code/ecdfz.R")
res = list()
for (i in 1:3) {
  z = zmat[i, ]
  p = (1 - pnorm(abs(z))) * 2
  bh.fd = sum(p.adjust(p, method = "BH") <= 0.05)
  pihat0.ash = get_pi0(ash(z, 1, method = "fdr"))
  ecdfz.fit = ecdfz.optimal(z)
  res[[i]] = list(z = z, p = p, bh.fd = bh.fd, pihat0.ash = pihat0.ash, ecdfz.fit = ecdfz.fit)
}
```
```{r, cache = TRUE, include = FALSE}
x = seq(-5, 5, 0.01)
H.x = sapply(1:8, EQL::hermite, x = x)
```

```{r, cache = TRUE, echo = FALSE}
K = c(7, 7, 7)
for (i in 1:3) {
  cat("Number of Discoveries:", res[[i]]$bh.fd, "; pihat0 =", res[[i]]$pihat0.ash, "\n")
  cat("Log-likelihood with N(0, 2):", sum(log(dnorm(res[[i]]$z, mean = 0, sd = sqrt(2)))), "\n")
  cat("Log-likelihood with Gaussian Derivatives:", -res[[i]]$ecdfz.fit$res[[K[i]]]$optimal_value + sum(log(dnorm(res[[i]]$z))), "\n")
  cat("Normalized weights:\n")
  w = res[[i]]$ecdfz.fit$res[[K[i]]]$primal_values[[1]]
  cat(rbind(paste(1:K[i], ":"), paste(w * sqrt(factorial(1:K[i])), ";")), sep = " ")
  hist(res[[i]]$z, breaks = 100, prob = TRUE, ylim = c(0, dnorm(0)), xlab = "z", main = "Histogram of z", xlim = range(x))
  lines(x, dnorm(x), col = "red")
  y = dnorm(x) * (H.x[, 1:K[i]] %*% res[[i]]$ecdfz.fit$res[[K[i]]]$primal_values[[1]] + 1)
  lines(x, y, col = "blue")
  legend("topright", col = c("red", "blue"), lty = 1, paste("K =", c(0, K[i])))

  tail = c(3, 5)

  cat("Zoom in to the positive tails:\n")
  hist(res[[i]]$z, breaks = 100, prob = TRUE, xlab = "z", main = "Histogram of z", xlim = sort(tail), ylim = c(0, dnorm(min(abs(tail)), 0, sqrt(2))))
  lines(x, dnorm(x), col = "red")
  lines(x, dnorm(x, 0, sqrt(2)), col = "red", lty = 2)
  y = dnorm(x) * (H.x[, 1:K[i]] %*% res[[i]]$ecdfz.fit$res[[K[i]]]$primal_values[[1]] + 1)
  lines(x, y, col = "blue")
  legend("topright", col = c("red", "blue", "red"), lty = c(1, 1, 2), c("N(0, 1)", paste("K =", K[i]), "N(0, 2)"))

  cat("Zoom in to the negative tails:\n")
  hist(res[[i]]$z, breaks = 100, prob = TRUE, xlab = "z", main = "Histogram of z", xlim = sort(-tail), ylim = c(0, dnorm(min(abs(tail)), 0, sqrt(2))))
  lines(x, dnorm(x), col = "red")
  lines(x, dnorm(x, 0, sqrt(2)), col = "red", lty = 2)
  y = dnorm(x) * (H.x[, 1:K[i]] %*% res[[i]]$ecdfz.fit$res[[K[i]]]$primal_values[[1]] + 1)
  lines(x, y, col = "blue")
  legend("topleft", col = c("red", "blue", "red"), lty = c(1, 1, 2), c("N(0, 1)", paste("K =", K[i]), "N(0, 2)"))

  qqnorm(res[[i]]$z, main = "Normal Q-Q plot for z scores")
  abline(0, 1)
  
  m = n
  pj = sort(res[[i]]$p)
  plot(pj, xlab = "Order", ylab = "Ordered p value", main = "All p values")
  abline(0, 1 / m, col = "blue")
  abline(0, 0.05 / m, col = "red")
  legend("top", lty = 1, col = c("blue", "red"), c("Uniform", "BH, FDR = 0.05"))

  plot(pj[1:max(100, res[[i]]$bh.fd)], xlab = "Order", ylab = "Ordered p value", main = "Zoom-in to 100 smallest p values", ylim = c(0, pj[max(100, res[[i]]$bh.fd)]))
  abline(0, 1 / m, col = "blue")
  points(pj[1:res[[i]]$bh.fd], col = "green", pch = 19)
  abline(0, 0.05 / m, col = "red")
  legend("top", lty = 1, col = c("blue", "red"), c("Uniform", "BH, FDR = 0.05"))
  
  plot(pj[pj <= 0.01], xlab = "Order", ylab = "Ordered p value", main = "Zoom-in to p values <= 0.01", ylim = c(0, 0.01))
  abline(0, 1 / m, col = "blue")
  points(pj[1:res[[i]]$bh.fd], col = "green", pch = 19)
  abline(0, 0.05 / m, col = "red")
  legend("top", lty = 1, col = c("blue", "red"), c("Uniform", "BH, FDR = 0.05"))
  
  plot(pj[pj <= 0.05], xlab = "Order", ylab = "Ordered p value", main = "Zoom-in to p values <= 0.05", ylim = c(0, 0.05))
  abline(0, 1 / m, col = "blue")
  points(pj[1:res[[i]]$bh.fd], col = "green", pch = 19)
  abline(0, 0.05 / m, col = "red")
  legend("top", lty = 1, col = c("blue", "red"), c("Uniform", "BH, FDR = 0.05"))
}
```

## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
