---
title: "Empirical Null with Gaussian Derivatives: Weight Constraints"
author: "Lei Sun"
date: 2017-03-27
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->


## Numerical issues

When fitting Gaussian derivatives, [overfitting](gaussian_derivatives_3.rmd) or [heavy correlation](gaussian_derivatives_4.html) may cause numerical instability.  Most of the time, the numerical instability leads to ridiculous weights $w_k$, making the fitted density break [the nonnegativity constraint](gaussian_derivatives.html#assumption_2:_we_further_assume_that_the_number_of_observations_(n)_is_sufficiently_large,_so_that_we_can_use_the_(n)_observed_(z)_scores_in_place_of_the_intractable_all_(xinmathbb{r})_in_the_second_constraint) for $x\neq z_i$'s.

Let normalized weights $W_k^s = W_k\sqrt{k!}$.  As [shown previously](gaussian_derivatives.html), under correlated null, the variance $\text{var}(W_k^s) = \alpha_k = \bar{\rho_{ij}^k}$.  Thus, under correlated null, the Gaussian derivative decomposition of the empirical distribution should have "reasonable" weights of similar decaying patterns.  In other words, $W_k^s$ with mean $0$ and variance $\bar{\rho_{ij}^k}$, should be in the order of $\sqrt{\rho^K}$ with a $\rho\in (0, 1)$.

## Examples

This example shows that numerical instability is reflected in the number of Gaussian derivatives fitted $K$ being too large, as well as in the normalized fitted weights of Gaussian derivatives $\hat w_k\sqrt{k!}$ being completely out of order $\sqrt{\rho^K}$.

```{r, message = FALSE}
source("../code/ecdfz.R")
```

```{r, cache = TRUE}
z = read.table("../output/z_null_liver_777.txt")
p = read.table("../output/p_null_liver_777.txt")
```

```{r, cache = TRUE, result = "hide", message = FALSE}
library(ashr)
DataSet = c(522, 724)
res_DataSet = list()
for (i in 1:length(DataSet)) {
  zscore = as.numeric(z[DataSet[i], ])
  fit.ecdfz = ecdfz.optimal(zscore)
  fit.ash = ash(zscore, 1, method = "fdr")
  fit.ash.pi0 = get_pi0(fit.ash)
  pvalue = as.numeric(p[DataSet[i], ])
  fd.bh = sum(p.adjust(pvalue, method = "BH") <= 0.05)
  res_DataSet[[i]] = list(DataSet = DataSet[i], fit.ecdfz = fit.ecdfz, fit.ash = fit.ash, fit.ash.pi0 = fit.ash.pi0, fd.bh = fd.bh, zscore = zscore, pvalue = pvalue)
}
```

```{r, cache = TRUE, result = "hide", message = FALSE}
library(EQL)
x.pt = seq(-5, 5, 0.01)
H.pt = sapply(1:15, EQL::hermite, x = x.pt)
```

```{r, cache = TRUE, echo = FALSE}
cat("Data Set", DataSet[2], ": Number of BH's False Discoveries:", res_DataSet[[2]]$fd.bh, "; ASH's pihat0 =", res_DataSet[[2]]$fit.ash.pi0)
cat("Normalized Weights for K = 8:\n")
cat(rbind(paste(1:8, ":"), paste(res_DataSet[[2]]$fit.ecdfz$res[[8]]$primal_values[[1]] * sqrt(factorial(1:8)), ";")), sep = " ")
cat("Normalized Weights for K = 14:\n")
cat(rbind(paste(1:14, ":"), paste(res_DataSet[[2]]$fit.ecdfz$res[[14]]$primal_values[[1]] * sqrt(factorial(1:14)), ";")), sep = " ")
ymax = max(dnorm(0), max(hist(res_DataSet[[2]]$zscore, plot = FALSE, breaks = 100)$density))
hist(res_DataSet[[2]]$zscore, prob = TRUE, breaks = 100, xlab = "z score", main = bquote("Histogram of Data Set"~.(res_DataSet[[2]]$DataSet)), ylim = c(0, ymax))
y.stdnormal = dnorm(x.pt)
lines(x.pt, y.stdnormal, col = "red")
y.pt.bad = (cbind(H.pt[, 1:14]) %*% res_DataSet[[2]]$fit.ecdfz$res[[14]]$primal_values[[1]] + 1) * y.stdnormal
lines(x.pt, y.pt.bad, col = "blue")
y.pt.better = (cbind(H.pt[, 1:8]) %*% res_DataSet[[2]]$fit.ecdfz$res[[8]]$primal_values[[1]] + 1) * y.stdnormal
lines(x.pt, y.pt.better, col = "green")
legend("topleft", lty = 1, col = c("blue", "green"), c("K = 14", "K = 8"), ncol = 2, seg.len = 1.25)
plot(0:res_DataSet[[2]]$fit.ecdfz$optimal$ord.fitted, res_DataSet[[2]]$fit.ecdfz$optimal$log.lik.gd, xlab = "Highest order of Gaussian derivatives K", ylab = expression(paste("Optimal objective ", hat(g)[K])), main = "Log-likelihood increase from N(0, 1)")
abline(v = 8, lty = 3, col = "green")
abline(h = res_DataSet[[2]]$fit.ecdfz$optimal$log.lik.gd[8 + 1], lty = 3, col = "green")
points(8, res_DataSet[[2]]$fit.ecdfz$optimal$log.lik.gd[8 + 1], col = "green", pch = 19)
abline(v = 14, lty = 3, col = "blue")
abline(h = res_DataSet[[2]]$fit.ecdfz$optimal$log.lik.gd[14 + 1], lty = 3, col = "blue")
points(14, res_DataSet[[2]]$fit.ecdfz$optimal$log.lik.gd[14 + 1], col = "blue", pch = 19)
legend("topleft", pch = 19, col = c("blue", "green"), legend = c("K = 14", "K = 8"))
```

## Weight constraints

Therefore, we can impose regularity in the fitted Gaussian derivatives by imposing constraints on $w_k$.  A good set of weights should have following properties.

1. They should make $\sum\limits_{k = 1}^\infty w_kh_k(x) + 1$ non-negative for $\forall x\in\mathbb{R}$.  This constraint needs to be satisfied for any distribution.
2. They should decay in roughly exponential order such that $w_k^s = w_k\sqrt{k!} \sim \sqrt{\rho^K}$.  This constraint needs to be satisfied particularly for empirical correlated null.
3. $w_k$ vanishes to essentially $0$ for sufficiently large $k$.  This constraint can be seen as coming from the last one, leading to the simplification that only first $K$ orders of Gaussian derivatives are enough.

As [Prof. Peter McCullagh](https://galton.uchicago.edu/~pmcc/) pointed out during a chat, there should be a rich literature discussing the non-negativity / positivity condition for Gaussian derivative decomposition, also known as [Edgeworth expansion](https://en.wikipedia.org/wiki/Edgeworth_series#Disadvantages_of_the_Edgeworth_expansion).  **This could potentially be a direction to look at.**

An approximation to the non-negativity constraint may come from [the fact](gaussian_derivatives.html#gaussian_derivatives_and_hermite_polynomials) that due to orthogonality of Hermite polynomials,

$$
W_k = \frac{1}{k!}\int_{-\infty}^\infty f_0(x)h_k(x)dx \ .
$$
Therefore, if $f_0$ is truly a proper density,

$$
\begin{array}{rclcl}
W_1 &=& \frac{1}{1!}\int_{-\infty}^\infty f_0(x)h_1(x)dx = \int_{-\infty}^\infty x f_0(x)dx &=& E[X]_{F_0} \\
W_2 &=& \frac{1}{2!}\int_{-\infty}^\infty f_0(x)h_1(x)dx = \frac12\int_{-\infty}^\infty (x^2-1) f_0(x)dx &=& \frac12E[X^2]_{F_0} -\frac12\\
W_3 &=& \frac{1}{3!}\int_{-\infty}^\infty f_0(x)h_1(x)dx = \frac16\int_{-\infty}^\infty (x^3-3x) f_0(x)dx &=& \frac16E[X^3]_{F_0} -\frac12E[X]_{F_0}\\
W_4 &=& \frac{1}{4!}\int_{-\infty}^\infty f_0(x)h_1(x)dx = \frac1{24}\int_{-\infty}^\infty (x^4-6x^2+3) f_0(x)dx &=& \frac1{24}E[X^4]_{F_0} -\frac14E[X^2]_{F_0} + \frac18\\
&\vdots&
\end{array}
$$
Note that $F_0$ is not the empirical cdf $\hat F$, even if we've taken into consideration that $N$ is sufficiently large, and the correlation structure is solely determined by $W_k$'s and Gaussian derivatives.  $F_0$ is inherently continuous, whereas the empirical cdf $\hat F$ is inherently non-differentiable.  This implies that the mean of $F_0$, $E[X]_{F_0} \neq \bar X$, the mean of the empirical cdf $\hat F$.  $W_k$'s are still parameters of $F_0$ that are not readily determined even given the observations (hence given the empirical cdf).

This relationship inspires the following two ways to constraint $W_k$'s.

## Method of moments estimates

Instead of using maxmimum likelihood estimates of $f_0$, that is, $\max\sum\limits_{i = 1}^n\log\left(\sum\limits_{k = 1}^\infty w_kh_k(z_i) + 1\right)$, we may use method of moments estimates:

$$
\begin{array}{rcl}
w_1 &=& \hat E[X]_{F_0}\\
w_2 &=& \frac12\hat E[X^2]_{F_0} -\frac12\\
w_3 &=& \frac16\hat E[X^3]_{F_0} -\frac12\hat E[X]_{F_0}\\
w_4 &=& \frac1{24}\hat E[X^4]_{F_0} -\frac14\hat E[X^2]_{F_0} + \frac18\\
&\vdots&\\
w_k &=& \cdots
\end{array}
$$

## Constraining weights by moments

Another way is to constraint the weights by the properties of the moments to prevent them going crazy, such that

$$
W_2 = \frac12E[X^2]_{F_0} -\frac12 \Rightarrow w_2 \geq -\frac12 \ .
$$

We may also combine the moment estimates and constraints like

$$
\begin{array}{rcl}
w_1 &=& \hat E[X]_{F_0}\\
w_2 &\geq& -\frac12\\
&\vdots&
\end{array}
$$
## Quantile Constraints

It has been shown [here](FDR_Null.html#result), [here](FDR_null_betahat.html#result), [here](correlated_z_3.html#result) that Benjamini-Hochberg (BH) is suprisingly robust under correlation.  So Matthew has an idea to constrain the quantiles of the estimated empirical null.  In his words,

> I wonder if you can constrain the quantiles of the estimated null distribution using something based on BH. Eg, the estimated quantiles should be no more than $1/\alpha$ times more extreme than expected under $N(0,1)$ where $\alpha$ is chosen to be the level you'd like to control FDR at under the global null.

> This is not a very specific idea, but maybe gives you an idea of the kind of constraint I mean... **Something to stop that estimated null having too extreme tails.**

The good thing is this idea of constraining quantiles is easy to implement.

$$
\begin{array}{rcl}
F_0(x) &=& \displaystyle\Phi(x) + \sum\limits_{k = 1}^\infty W_k(-1)^k
\varphi^{(k - 1)}(x)\\
&=&
\displaystyle\Phi(x) - \sum\limits_{k = 1}^\infty W_kh_{k-1}(x)\varphi(x)
\end{array}
$$
So the constraints that $F_0(x) \leq g(\alpha)$ for certain $x, g, \alpha$ are essentially linear constraints on $W_k$.  Thus the convexity of the problem is preserved.

## Implementation

Right now using the current implementation the results are disappointing.  Using the constraint on $w_2$ usually makes the optimization unstable.  Using moment estimates on $w_1$ and $w_2$ performs better, but only sporadically.

Here we are re-running the optimization on [the previous $\rho_{ij} \equiv 0.7$ example](gaussian_derivatives_4.html#fitting_experiments_when_(rho_{ij})_is_large) with $\hat w_1$ and $\hat w_2$ estimated by the method of moments.  Without this tweak current implementation can only work up to $K = 3$, which is obviously insufficient.  Now it can go up to $K = 6$.  $K = 7$ doesn't look good, although making some improvement compared with $K = 6$.

The results are not particularly encouraging.  Right now we'll leave it here.

```{r, result = "hide"}
rho = 0.7
set.seed(777)
n = 1e4
z = rnorm(1) * sqrt(rho) + rnorm(n) * sqrt(1 - rho)
w.3 = list()
for (ord in 6:7) {
  H = sapply(1 : ord, EQL::hermite, x = z)
  w <- Variable(ord - 2)
  w1 = mean(z)
  w2 = mean(z^2) / 2 - .5
  H2w = H[, 1:2] %*% c(w1, w2)
  objective <- Maximize(CVXR::sum_entries(CVXR::log1p(H[, 3:ord] %*% w + H2w)))
  prob <- Problem(objective)
  capture.output(result <- solve(prob), file = "/dev/null")
  w.3[[ord - 5]] = c(w1, w2, result$primal_values[[1]])
}
```


```{r, cache = TRUE, include = FALSE}
x = seq(-6, 6, 0.01)
H.x = sapply(1 : 7, EQL::hermite, x = x)
```

```{r, cache = TRUE, echo = FALSE}
hist(z, breaks = 100, prob = TRUE, xlim = range(x))
lines(x, dnorm(x), col = "red")
lines(x, dnorm(x) * (1 + H.x[, 1:6] %*% w.3[[1]]), col = 3)
lines(x, dnorm(x) * (1 + H.x[, 1:7] %*% w.3[[2]]), col = 4)
legend("topright", lty = 1, col = c(2:4), c("K = 0", "K = 6", "K = 7"))
```

## Conclusion

Weights $w_k$ are of central importance in the algorithm.  They contain the information regarding whether the composition of Gaussian derivatives is a proper density function, and if it is, whether it's a correlated null.  We need to find an ingenious way to impose appropriate constraints on $w_k$.

## Appendix: implementation of moments constraints

```{r, eval = FALSE, include = FALSE}
library(cvxr)
library(EQL)


## constraint on 2
rho = 0.7
ord = 4

set.seed(777)
n = 1e4
z = rnorm(1) * sqrt(rho) + rnorm(n) * sqrt(1 - rho)


H = sapply(1 : ord, EQL::hermite, x = z)
w <- Variable(ord)
c1 <- c(1, rep(0, ord - 1))
c2 <- c(0, 1, rep(0, ord - 2))
mu_z = mean(z)
objective <- Maximize(CVXR::sum_entries(CVXR::log1p(H %*% w)))
constraint <- list(CVXR::sum_entries(c2 * w) >= -0.5, CVXR::sum_entries(c1 * w) == mu_z)
prob <- Problem(objective, constraint)
capture.output(result <- solve(prob), file = "/dev/null")
result$primal_values[[1]]

x = seq(-4, 4, 0.01)
H.x = sapply(1 : ord, EQL::hermite, x = x)
hist(z, breaks = 100, prob = TRUE, xlim = range(x))
lines(x, dnorm(x), col = "red")
y = dnorm(x) * (H.x %*% result$primal_values[[1]] + 1)
lines(x, y, col = "blue")



## Mom on 1 & constraint on 2
rho = 0.7
ord = 3

set.seed(777)
n = 1e4
z = rnorm(1) * sqrt(rho) + rnorm(n) * sqrt(1 - rho)
H = sapply(1 : ord, EQL::hermite, x = z)
w <- Variable(ord - 1)
c2 <- c(1, rep(0, ord - 2))
w1 = mean(z)
H1w = H[, 1] * w1
objective <- Maximize(CVXR::sum_entries(CVXR::log1p(H[, 2:ord] %*% w + H1w)))
constraint <- list(CVXR::sum_entries(c2 * w) >= -0.5)
prob <- Problem(objective, constraint)
capture.output(result <- solve(prob), file = "/dev/null")
result$primal_values[[1]]

x = seq(-4, 4, 0.01)
H.x = sapply(1 : ord, EQL::hermite, x = x)
hist(z, breaks = 100, prob = TRUE, xlim = range(x))
lines(x, dnorm(x), col = "red")
y = dnorm(x) * (H.x %*% c(w1, result$primal_values[[1]]) + 1)
lines(x, y, col = "blue")


e = 1.5
s = sqrt(1 + e^2)
z = rnorm(n, 0, s)
source("code/ecdfz.R")
fit = ecdfz.optimal(z)
x = seq(-6, 6, 0.01)
H.x = sapply(1 : fit$optimal$ord.fitted, EQL::hermite, x = x)
hist(z, breaks = 100, prob = TRUE, xlim = range(x), ylim = c(0, dnorm(0)))
lines(x, dnorm(x), col = "red")
y = dnorm(x) * (H.x[, 1:3] %*% fit$res[[3]]$primal_values[[1]] + 1)
lines(x, y, col = "blue")
```

## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
