---
title: "A closer look at `Knockoff` on heavy correlation design"
author: "Lei Sun"
date: 2018-02-20
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

## Introduction

`Knockoff` has 3 steps.

1. Generate knockoff variables, which keep the same correlation structure as original variables but has no effect on the response.

2. Generate test statistics such that these statistics tend to be large positive number for non-null variables but iid positive or negative for null variables.

3. Find a cutoff threshold for those test statistics to control the estimated FDR under $q$.

The default `knockoff::knockoff.filter` function uses `SDP` construction in step 1 and `LASSO`-related statistics in step 2. However, [we've found](knockoff_5.html#scenario_3:_(hatbeta)_from_a_factor_model7) that `Knockoff` coded in this way failed to control FDR in simulations when variables are generated such that $\hat\beta$ has heavy average absolute pairwise correlation, which seems to contradict Thoerem 2 in the `Knockoff` paper. Now we take a closer look to see what went wrong.

In step 1, we use two construction methods: `equi` and `sdp`. `sdp` is believed to be more powerful. In step 2, we use two statistics: `marginal` and `lasso`-related. `lasso`-related is believed to be more powerful.

```{r, echo = FALSE, message = FALSE}
library(knockoff)
library(reshape2)
library(ggplot2)
library(doMC)
library(lattice)
fdp <- function(selected) sum(beta[selected] == 0) / max(length(selected), 1)
power <- function(selected, k = k) sum(beta[selected] != 0) / max(k, 1)
```

```{r}
n <- 3e3
p <- 1e3
k <- 50
d <- 7
q <- 0.1
```

## Fixed $X$ Knockoffs: 1000 simulation trials

```{r, echo = FALSE, cache = TRUE}
set.seed(777)
## Correlation matrix of betahat
B <- matrix(rnorm(p * d, 0, 1), p, d)
Sigma.betahat <- tcrossprod(B) + diag(p)
Cor.betahat <- cov2cor(Sigma.betahat)
## Simulate X with Cor.betahat^{-1} correlation structure
X <- matrix(rnorm(n * p), n, p) %*% chol(cov2cor(solve(Cor.betahat)))
## Centering X
X <- t(t(X) - colMeans(X))
## Normalize X
X <- t(t(X) / sqrt(colSums(X^2)))
## Generate knockoffs
Xk <- knockoff::create.fixed(X, method = "equi")
Xk_e <- Xk$Xk
Xk <- knockoff::create.fixed(X, method = "sdp")
Xk_s <- Xk$Xk
## Signal strength
A <- round(3 * sqrt(mean(diag(solve(crossprod(X))))))
## Set beta
beta <- rep(0, p)
nonzero <- sample(p, k)
beta[nonzero] <- A
```

```{r, echo = FALSE, cache = TRUE}
m <- 1000
W <- result <- list()
for (i in 1 : m) {
y <- X %*% beta + rnorm(n)

## Least squares
lm.fit <- lm(y ~ X - 1)
pvalue <- summary(lm.fit)$coefficients[, 4]

## BH
BH.fit <- p.adjust(pvalue, method = "BH")
BH.selected <- (1 : p)[BH.fit <= q]

## Knockoff
W[[i]] <- cbind(
  equi_lasso <- knockoff::stat.glmnet_lambdasmax(X, Xk_e, y),
  equi_marginal <- abs(t(X) %*% y) - abs(t(Xk_e) %*% y),
  sdp_lasso <- knockoff::stat.glmnet_lambdasmax(X, Xk_s, y),
  sdp_marginal <- abs(t(X) %*% y) - abs(t(Xk_s) %*% y)
)

Knockoff.threshold <- apply(W[[i]], 2, knockoff.threshold, fdr = q, offset = 0)
Knockoff.threshold.plus <- apply(W[[i]], 2, knockoff.threshold, fdr = q, offset = 1)

fdp.Knockoff <- fdp.Knockoff.plus <- power.Knockoff <- power.Knockoff.plus <- c()
for (j in 1 : ncol(W[[i]])) {
  fdp.Knockoff[j] <- fdp(which(W[[i]][, j] >= Knockoff.threshold[j]))
  fdp.Knockoff.plus[j] <- fdp(which(W[[i]][, j] >= Knockoff.threshold.plus[j]))
  power.Knockoff[j] <- sum(beta[which(W[[i]][, j] >= Knockoff.threshold[j])] != 0) / max(k, 1)
  power.Knockoff.plus[j] <- sum(beta[which(W[[i]][, j] >= Knockoff.threshold.plus[j])] != 0) / max(k, 1)
}

result[[i]] <- c(
  fdp(BH.selected), fdp.Knockoff, fdp.Knockoff.plus,
  power(BH.selected, k), power.Knockoff, power.Knockoff.plus
  )
}
```

```{r, echo = FALSE}
result.matrix <- matrix(unlist(result), m, byrow = TRUE)
boxplot(result.matrix[, 1 : 9], ylab = "FDP", las = 2, names = c(
  "BH",
  "KO.Equi.LASSO",
  "KO.Equi.Marginal",
  "KO.SDP.LASSO",
  "KO.SDP.Marginal",
  "KO+.Equi.LASSO",
  "KO+.Equi.Marginal",
  "KO+.SDP.LASSO",
  "KO+.SDP.Marginal"
  ), cex.axis = 0.55
)
points(colMeans(result.matrix[, 1 : 9]), col = "blue", pch = 3)
abline(h = q, lty = 2, col = "red")
```

```{r, echo = FALSE, message = FALSE}
boxplot(result.matrix[, 10 : 18], ylab = "TDP", las = 2, names = c(
  "BH",
  "KO.Equi.LASSO",
  "KO.Equi.Marginal",
  "KO.SDP.LASSO",
  "KO.SDP.Marginal",
  "KO+.Equi.LASSO",
  "KO+.Equi.Marginal",
  "KO+.SDP.LASSO",
  "KO+.SDP.Marginal"
  ), cex.axis = 0.55
)
points(colMeans(result.matrix[, 10 : 18]), col = "blue", pch = 3)

null <- setdiff(seq(p), nonzero)
W.sign <- lapply(W, apply, 2, function(x)(sum(x[null] > 0) / sum(x[null] != 0)))
W.sign <- matrix(unlist(W.sign), m, 4, byrow = TRUE)
colnames(W.sign) <- c(
  "equi + stat.glmnet_lambdasmax",
  "equi + marginaldiff",
  "sdp + stat.glmnet_lambdasmax",
  "sdp + marginaldiff"
)
ggplot2::ggplot(data = reshape2::melt(as.data.frame(W.sign)), mapping = aes(x = value)) +
  geom_histogram(breaks = seq(0, 1, by = 0.05)) +
  facet_wrap(~ variable) +
  labs(x = "Proportion of Null Test Statistics Being Positive") +
  geom_vline(xintercept = 0.5, col = "red", linetype = "dashed")
```

## Model $X$ Knockoffs: 1000 simulation trials

```{r, echo = FALSE, cache = TRUE}
set.seed(777)

## Covariance matrix of betahat
B <- matrix(rnorm(p * d, 0, 1), p, d)
Sigma.betahat <- tcrossprod(B) + diag(p)
## Covariance matrix of X = cov2cor(Sigma.betahat^{-1})
Sigma.X <- cov2cor(solve(Sigma.betahat))
## Set beta
A <- 10
beta <- rep(0, p)
nonzero <- sample(p, k)
beta[nonzero] <- A / sqrt(n)
```

```{r, echo = FALSE, cache = TRUE, message = FALSE, warning = FALSE}
m <- 1000
W <- result <- list()
for (i in 1 : m) {
## Generate the variables from a multivariate normal distribution
X <- matrix(rnorm(n * p), n, p) %*% chol(Sigma.X)
## Generate knockoffs
Xk_e <- knockoff::create.second_order(X, method = "equi")
Xk_s <- knockoff::create.second_order(X, method = "sdp")

## Generate data
y <- X %*% beta + rnorm(n)

## Least squares
lm.fit <- lm(y ~ X - 1)
pvalue <- summary(lm.fit)$coefficients[, 4]

## BH
BH.fit <- p.adjust(pvalue, method = "BH")
BH.selected <- (1 : p)[BH.fit <= q]

## Knockoff
W[[i]] <- cbind(
  equi_lasso <- knockoff::stat.glmnet_coefdiff(X, Xk_e, y),
  equi_marginal <- abs(t(X) %*% y) - abs(t(Xk_e) %*% y),
  sdp_lasso <- knockoff::stat.glmnet_coefdiff(X, Xk_s, y),
  sdp_marginal <- abs(t(X) %*% y) - abs(t(Xk_s) %*% y)
)

Knockoff.threshold <- apply(W[[i]], 2, knockoff.threshold, fdr = q, offset = 0)
Knockoff.threshold.plus <- apply(W[[i]], 2, knockoff.threshold, fdr = q, offset = 1)

fdp.Knockoff <- fdp.Knockoff.plus <- power.Knockoff <- power.Knockoff.plus <- c()
for (j in 1 : ncol(W[[i]])) {
  fdp.Knockoff[j] <- fdp(which(W[[i]][, j] >= Knockoff.threshold[j]))
  fdp.Knockoff.plus[j] <- fdp(which(W[[i]][, j] >= Knockoff.threshold.plus[j]))
  power.Knockoff[j] <- sum(beta[which(W[[i]][, j] >= Knockoff.threshold[j])] != 0) / max(k, 1)
  power.Knockoff.plus[j] <- sum(beta[which(W[[i]][, j] >= Knockoff.threshold.plus[j])] != 0) / max(k, 1)
}

result[[i]] <- c(
  fdp(BH.selected), fdp.Knockoff, fdp.Knockoff.plus,
  power(BH.selected, k), power.Knockoff, power.Knockoff.plus
  )
}
```

```{r, echo = FALSE}
result.matrix <- matrix(unlist(result), m, byrow = TRUE)
boxplot(result.matrix[, 1 : 9], ylab = "FDP", las = 2, names = c(
  "BH",
  "KO.Equi.LASSO",
  "KO.Equi.Marginal",
  "KO.SDP.LASSO",
  "KO.SDP.Marginal",
  "KO+.Equi.LASSO",
  "KO+.Equi.Marginal",
  "KO+.SDP.LASSO",
  "KO+.SDP.Marginal"
  ), cex.axis = 0.55
)
points(colMeans(result.matrix[, 1 : 9]), col = "blue", pch = 3)
abline(h = q, lty = 2, col = "red")
```

```{r, echo = FALSE}
boxplot(result.matrix[, 10 : 18], ylab = "TDP", las = 2, names = c(
  "BH",
  "KO.Equi.LASSO",
  "KO.Equi.Marginal",
  "KO.SDP.LASSO",
  "KO.SDP.Marginal",
  "KO+.Equi.LASSO",
  "KO+.Equi.Marginal",
  "KO+.SDP.LASSO",
  "KO+.SDP.Marginal"
  ), cex.axis = 0.55
)
points(colMeans(result.matrix[, 10 : 18]), col = "blue", pch = 3)
```

```{r, echo = FALSE, message = FALSE}
null <- setdiff(seq(p), nonzero)
W.sign <- lapply(W, apply, 2, function(x)(sum(x[null] > 0) / sum(x[null] != 0)))
W.sign <- matrix(unlist(W.sign), m, 4, byrow = TRUE)
colnames(W.sign) <- c(
  "equi + stat.glmnet_coefdiff",
  "equi + marginaldiff",
  "sdp + stat.glmnet_coefdiff",
  "sdp + marginaldiff"
)
ggplot2::ggplot(data = reshape2::melt(as.data.frame(W.sign)), mapping = aes(x = value)) +
  geom_histogram(breaks = seq(0, 1, by = 0.05)) +
  facet_wrap(~ variable) +
  labs(x = "Proportion of Null Test Statistics Being Positive") +
  geom_vline(xintercept = 0.5, col = "red", linetype = "dashed")
```

## Observation

A set of well-bahaving `Knockoff` variables $X^k$ should have the property that
$$
\begin{array}{c}
cor(X^k_i, X^k_j) = cor(X_i, X_j)\\
cor(X_i, X^k_j) = cor(X_i, X_j)
\end{array}
$$
while $cor(X_i, X^k_i)$ should be as small as possible. It turns out it's just not that easy to generate these well-behaving `Knockoff` variables when columns in $X$ are correlated in a certain way. Especially when using `SDP` optimization, it could generate a lot of knockoffs that are exactly the same as the originals.

```{r, echo = FALSE}
set.seed(777)
## Correlation matrix of betahat
B <- matrix(rnorm(p * d, 0, 1), p, d)
Sigma.betahat <- tcrossprod(B) + diag(p)
Cor.betahat <- cov2cor(Sigma.betahat)
## Simulate X with Cor.betahat^{-1} correlation structure
X <- matrix(rnorm(n * p), n, p) %*% chol(cov2cor(solve(Cor.betahat)))
## Centering X
X <- t(t(X) - colMeans(X))
## Normalize X
X <- t(t(X) / sqrt(colSums(X^2)))
## Generate knockoffs
Xk <- knockoff::create.fixed(X, method = "equi")
Xk_e <- Xk$Xk
Xk <- knockoff::create.fixed(X, method = "sdp")
Xk_s <- Xk$Xk
corxxke <- corxxks <- c()
for (i in 1 : p) {
  corxxke[i] <- cor(X[, i], Xk_e[, i])
  corxxks[i] <- cor(X[, i], Xk_s[, i])
}
corxxk <- cbind.data.frame(method <- c(rep("equi", p), rep("sdp", p)),
                           cor <- c(corxxke, corxxks))
histogram(~ cor | method, xlab = expression(Cor(X[j], tilde(X)[j])))
```

If an original variable and its knockoff are too similar, it essentially makes little difference which one is included in the model, from a goodness of fit point of view.

The problem becomes more severe when we fit models like LASSO using methods like coordinate descent. The result depends in large part on the sequence of variables getting into the model. So if we feed LASSO with `cbind(X, Xk)`, for every iteration, it's always `X[j]` being optimized before `Xk[j]`. That's a major reason we see asymmetric test statistics as above, and why `Knockoff` loses FDR control in these circumstances.

One way to fix that is to randomize the order of variables in `cbind(X, Xk)` before feeding them to LASSO. The following is a simulation.

```{r, echo = FALSE, cache = TRUE}
set.seed(777)
## Correlation matrix of betahat
B <- matrix(rnorm(p * d, 0, 1), p, d)
Sigma.betahat <- tcrossprod(B) + diag(p)
Cor.betahat <- cov2cor(Sigma.betahat)
## Simulate X with Cor.betahat^{-1} correlation structure
X <- matrix(rnorm(n * p), n, p) %*% chol(cov2cor(solve(Cor.betahat)))
## Centering X
X <- t(t(X) - colMeans(X))
## Normalize X
X <- t(t(X) / sqrt(colSums(X^2)))
## Generate knockoffs
Xk <- knockoff::create.fixed(X, method = "equi")
Xk_e <- Xk$Xk
Xk <- knockoff::create.fixed(X, method = "sdp")
Xk_s <- Xk$Xk
## Signal strength
A <- round(3 * sqrt(mean(diag(solve(crossprod(X))))))
## Set beta
beta <- rep(0, p)
nonzero <- sample(p, k)
beta[nonzero] <- A

m <- 100
W <- result <- list()
for (i in 1 : m) {
  y <- X %*% beta + rnorm(n)

  ## Least squares
  lm.fit <- lm(y ~ X - 1)
  pvalue <- summary(lm.fit)$coefficients[, 4]

  ## BH
  BH.fit <- p.adjust(pvalue, method = "BH")
  BH.selected <- (1 : p)[BH.fit <= q]

  orig <- 1 : p
  random.order.variable <- sample(2 * p)
  Z <- knockoff:::lasso_max_lambda(cbind(X, Xk_e)[, random.order.variable], y, method = "glmnet", family = "gaussian")
  Z <- Z[order(random.order.variable)]
  W_e <- pmax(Z[orig], Z[orig + p]) * sign(Z[orig] - Z[orig + p])

  random.order.variable <- sample(2 * p)
  Z <- knockoff:::lasso_max_lambda(cbind(X, Xk_s)[, random.order.variable], y, method = "glmnet", family = "gaussian")
  Z <- Z[order(random.order.variable)]
  W_s <- pmax(Z[orig], Z[orig + p]) * sign(Z[orig] - Z[orig + p])

  ## Knockoff
  W[[i]] <- cbind(
    equi_marginal <- abs(t(X) %*% y) - abs(t(Xk_e) %*% y),
    sdp_marginal <- abs(t(X) %*% y) - abs(t(Xk_s) %*% y),
    equi_lasso <- knockoff::stat.glmnet_lambdasmax(X, Xk_e, y),
    equi_lasso_random <- W_e,
    sdp_lasso <- knockoff::stat.glmnet_lambdasmax(X, Xk_s, y),
    sdp_lasso_random <- W_s
  )

  Knockoff.threshold <- apply(W[[i]], 2, knockoff.threshold, fdr = q, offset = 0)
  Knockoff.threshold.plus <- apply(W[[i]], 2, knockoff.threshold, fdr = q, offset = 1)

  fdp.Knockoff <- fdp.Knockoff.plus <- power.Knockoff <- power.Knockoff.plus <- c()
  for (j in 1 : ncol(W[[i]])) {
    fdp.Knockoff[j] <- fdp(which(W[[i]][, j] >= Knockoff.threshold[j]))
    fdp.Knockoff.plus[j] <- fdp(which(W[[i]][, j] >= Knockoff.threshold.plus[j]))
    power.Knockoff[j] <- sum(beta[which(W[[i]][, j] >= Knockoff.threshold[j])] != 0) / max(k, 1)
    power.Knockoff.plus[j] <- sum(beta[which(W[[i]][, j] >= Knockoff.threshold.plus[j])] != 0) / max(k, 1)
  }

  result[[i]] <- c(
    fdp(BH.selected), fdp.Knockoff, fdp.Knockoff.plus,
    power(BH.selected, k), power.Knockoff, power.Knockoff.plus
  )
}
```

```{r, echo = FALSE}
par(mar = c(9, 4.5, 0.5, 0.5))
result.matrix <- matrix(unlist(result), m, byrow = TRUE)
boxplot(result.matrix[, 1 : 13], ylab = "FDP", las = 2, names = c(
  "BH",
  "KO.Equi.Marginal",
  "KO.SDP.Marginal",
  "KO.Equi.LASSO",
  "KO.Equi.LASSO.Random",
  "KO.SDP.LASSO",
  "KO.SDP.LASSO.Random",
  "KO+.Equi.Marginal",
  "KO+.SDP.Marginal",
  "KO+.Equi.LASSO",
  "KO+.Equi.LASSO.Random",
  "KO+.SDP.LASSO",
  "KO+.SDP.LASSO.Random"
), cex.axis = 0.75, border = c("green", rep(c(rep("black", 4), rep("red", 2)), 2))
)
points(colMeans(result.matrix[, 1 : 13]), col = "blue", pch = 3)
abline(h = q, lty = 2, col = "red")
abline(v = c(1.5, 7.5), col = "maroon")

boxplot(result.matrix[, 14 : 26], ylab = "TDP", las = 2, names = c(
  "BH",
  "KO.Equi.Marginal",
  "KO.SDP.Marginal",
  "KO.Equi.LASSO",
  "KO.Equi.LASSO.Random",
  "KO.SDP.LASSO",
  "KO.SDP.LASSO.Random",
  "KO+.Equi.Marginal",
  "KO+.SDP.Marginal",
  "KO+.Equi.LASSO",
  "KO+.Equi.LASSO.Random",
  "KO+.SDP.LASSO",
  "KO+.SDP.LASSO.Random"
), cex.axis = 0.75, border = c("green", rep(c(rep("black", 4), rep("red", 2)), 2))
)
points(colMeans(result.matrix[, 14 : 26]), col = "blue", pch = 3)
abline(v = c(1.5, 7.5), col = "maroon")

null <- setdiff(seq(p), nonzero)
W.sign <- lapply(W, apply, 2, function(x)(sum(x[null] > 0) / sum(x[null] != 0)))
W.sign <- matrix(unlist(W.sign), m, byrow = TRUE)
colnames(W.sign) <- c(
  "equi + marginaldiff",
  "sdp + marginaldiff",
  "equi + stat.glmnet_lambdasmax",
  "equi + stat.glmnet_lambdasmax + random",
  "sdp + stat.glmnet_lambdasmax",
  "sdp + stat.glmnet_lambdasmax + random"
)

ggplot2::ggplot(data = reshape2::melt(as.data.frame(W.sign)), mapping = aes(x = value)) +
  geom_histogram(breaks = seq(0, 1, by = 0.05)) +
  facet_wrap(~ variable, ncol = 2) +
  labs(x = "Proportion of Null Test Statistics Being Positive") +
  geom_vline(xintercept = 0.5, col = "red", linetype = "dashed")
```

After column randomization, the test statistcs for null variables are back to normal and `Knockoff` controls FDR again. The low power is another issue.

## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
