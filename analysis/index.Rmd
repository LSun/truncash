---
title: "Truncated Adaptive Shrinkage (`truncash`)"
output:
  html_document:
    toc: false
---

`truncash` (Truncated ASH) is an exploratory project with Matthew, built on [`ashr`].

* [Matthew's initial observation on null, correlated data](voom_null.html)

[Prof. Matthew Stephens](http://stephenslab.uchicago.edu/) did a quick investigation of the p values and z scores obtained for simulated null data (using just voom transform, no correction) from real RNA-seq data of [GTEx](http://www.gtexportal.org/home/).  Here is what he found.

"I found something that I hadn’t realized, although is obvious in hindsight: although you sometimes see inflation under null of $p$-values/$z$-scores, the most extreme values are not inflated compared with expectations (and tend to be deflated). That is the histograms of $p$-values that show inflation near $0$ (and deflation near $1$) actually hide something different going on in the very left hand side near $0$.  The qq-plots are clearer… showing most extreme values are deflated, or not inflated.  This is expected under positive correlation i think.  For example, if all $z$-scores were the same (complete correlation), then most extreme of n would just be $N(0,1)$. but if independent the most extreme of n would have longer tails..."

Matthew's initial observation inspired this project.  If under positive correlation, the most extreme tend to be not inflated, maybe we can use them to control the false discoveries.  Meanwhile, if the moderate are more prone to inflation due to correlation, maybe it's better to make only partial use of their information.

* [Occurrence of extreme observations](ExtremeOccurrence.html)

As [Prof. Michael Stein](https://galton.uchicago.edu/~stein/) pointed during a conversation with Matthew, if the marginal distribution is correct then the expected number exceeding any threshold should be correct.  So if the tail is "usually"" deflated, it should be that with some small probability there are many large $z$-scores (even in the tail).  Therefore, if "on average" we have the right number of large $z$-scores/small $p$-values, and "usually" we have too few, then "rarely" we should have too many.  A simulation is run to check this intuition.

* [Two FWER-controlling procedures on correlated null](StepDown.html)

In order to understand the behavior of $p$-values of top expressed, correlated genes under the global null, simulated from GTEx data, we apply two FWER-controlling multiple comparison procedures, Holm's "step-down" ([Holm 1979]) and Hochberg's "step-up." ([Hochberg 1988])

* [`truncash` Model and first simulations](truncash.html)

* [Pipeline for simulating null data](nullpipeline.html)

Using a toy model to examine and document the pipeline to simulate null summary statistics at each step, including `edgeR::calcNormFactors`, `limma::voom`, `limma::lmFit`, `limma::eBayes`.

* [FDR on Null, Part 1](FDR_Null.html)
* [FDR on Null, Part 2](FDR_null_betahat.html)

Apply two FDR-controlling procedures, BH and BY, as well as two $s$ value models, `ash` and `truncash` to the simulated, correlated null data, and compare the numbers of false discoveries (by definition, all discoveries should be false) obtained.  Part 1 uses $z$ scores only, Part 2 uses $\hat \beta$ and moderated $\hat s$.

* [$\hat\pi_0$ estimated in correlated global null](pihat0_null.html)

$\hat\pi_0$ estimated by `ash` and `truncash` with $T = 1.96$ on correlated global null data simulated from GTEx/Liver.  Ideally they should be close to $1$.

* [Ordered $p$ values vs critical values](cutoff_null.html)

For various FWER / FDR controlling procedures, and for `truncash`, what matters the most is the behavior of the most extreme observations.  Here these extreme $p$ values are plotted along with common critical values used by various procedures, in order to shed light on their behavior.

It's very exploratory.  May be related to Extreme Value Thoery and Concentration of Measure.  To be continued.

* [Single most extreme observation](SingleExtOb.html)

What will happen if we allow the threshold in `truncash` dependent on data?  Let's start from the case when we only know the single most extreme observation.


* [Handling $t$ likelihood](t-likelihood.html)

When moving to $t$ likelihood, or in other words, when taking randomness of $\hat s$ into consideration, things get trickier.  Here we review several techiniques currently used in Matthew's lab, regarding $t$ likelihood and uniform mixture priors, based on a discussion with Matthew.

* [Histogram of correlated $z$ scores, random data sets](correlated_z.html)
* [Histogram of correlated $z$ scores, `ash`-hostile data sets](correlated_z_2.html)
* [Histogram of correlated $z$ scores, `BH`-hostile data sets](correlated_z_3.html)

An implicit key question of this inquiry is: what's the behavior of $z$ scores under dependency?  We take a look at their histograms.  First for randomly sampled data sets.  Second for those most "hostile" to `ash`.  Third for those most "hostile" to `BH`.  The bottom line is we are reproducing what Efron observed in microarray data, that "the theoretical null may fail" in different ways.  Now the key questions are

1. Why the theoretical null may fail?  What does it mean by **correlation**?

2. Can `truncash` make `ash` more robust against some of the foes that make the theoretical null fail?

3. Generally, how robust is empirical Bayes? Is empirical Bayes robust or non-robust to certain kinds of correlation?

* [Fitting empirical null with Gaussian derivatives: Theory](gaussian_derivatives.html)
* [Fitting empirical null with Gaussian derivatives: Examples](gaussian_derivatives_2.html)
* [Fitting empirical null with Gaussian derivatives: Numerical issues](gaussian_derivatives_3.html)
* [Fitting empirical null with Gaussian derivatives: Large correlations](gaussian_derivatives_4.html)

Inspired by [Schwartzman 2010](http://amstat.tandfonline.com/doi/abs/10.1198/jasa.2010.tm10237), we experiment a new way to tackle "empirical null."

* [Fitting empirical null with Gaussian derivatives: Weight constraints](gaussian_derivatives_5.html)

In Gaussian derivatives decomposition, weights $W_k$ and especially [normalized weights](gaussian_derivatives_5.html#numerical_issues) $W_k^s = W_k\sqrt{k!}$ contain substantial information. In order to produce a proper density, and in order to regularize the fitted density to make it describe a plausible empiricall correlated null, constraints need to be imposed on the weights.

* [True signal vs correlated null: small effects](alternative.html)
* [True signal vs correlated null: larger effects](alternative2.html)

Both true effects and correlation can distort the empirical distribution away from the standard normal $N(0, 1)$, and Gaussian derivatives are presumably able to fit both.  Therefore, is there a way to let Gaussian derivatives with [a reasonable number of reasonable weights](gaussian_derivatives_5.html#weight_constraints) automatically identify correlated null from true effects?  It works well when effects are *not too small* right now.

* [Diagnostic plots for `ASH`](diagnostic_plot.html)
* [Diagnostic plots for `ASH` on correlated null](diagnostic_correlated_z.html)

Under the exchangeability assumption, the goodness of fit of empirical Bayes can be measured by the behavior of $\left\{\hat F_j = \hat F_{\hat\beta_j | \hat s_j}\left(\hat\beta_j\mid\hat s_j\right)\right\}$.  Meanwhile, if `ASH` is applied to correlated null $z$ scores, estimated $\hat g$ will not only be different from $\delta_0$; moreover, with this estimated $\hat g$, $\left\{\hat F_j\right\}$ might not behave like $\text{Unif}\left[0, 1\right]$.

[`ashr`]: https://github.com/stephens999/ashr

<!-- The goal of this new template is to simplify the setup and maintenance of a research website. -->
<!-- Specifically, -->

<!-- *  Easier to build and extend the website using the new tools in the [rmarkdown][] package and [latest RStudio release][rstudio] -->
<!-- *  Easier to deploy the website with Git and GitHub by only using one branch -->

<!-- [rmarkdown]: http://rmarkdown.rstudio.com/rmarkdown_websites.htm -->
<!-- [rstudio]: https://www.rstudio.com/products/rstudio/download/preview/ -->
