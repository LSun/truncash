---
title: "Pipeline for null simulation"
author: "Lei Sun"
date: 2017-02-13
output: html_document
---

```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

```{r knitr-opts-chunk, include=FALSE}
```

**Last updated:** `r Sys.Date()`

**Code version:** `r workflowr::extract_commit(".", 1)$sha1`

## Introduction

Up until now, we are using simulated correlated null data for exploration, and it would be helpful to examine and document the pipeline for generating these data, including $\hat\beta$, $\hat s$, $z$ score, $t$ statistics, $p$ value.

## Pipeline

An expression matrix $X_{G \times N}$ will go through the following steps to get measurements of length $g$: Choose the top $g$ expressed genes; randomly sample $n$ cases vs $n$ controls; calculate normalizing factor by `edgeR::calcNormFactors`; turn counts to $\log_2$; calculate variance weight by `limma::voom`.  We'll go through this pipeline with a toy data set.

```{r}
library(edgeR)
library(limma)
```

### read/generate raw counts
Suppose the raw counts are stored in a $10 \times 8$ matrix with $10$ genes and $8$ samples, counts simulated from Possion.

```{r}
set.seed(777)
r = matrix(rpois(n = 80, lambda = 5), nrow = 10)
r
```

### Select top expressed genes:
1. convert each count to its lcpm.
2. select top expressed genes according their sum of lcpm.

In this example we choose top $6$ genes from a total of $10$ genes.

```{r}
lcpm = function(r){R = colSums(r); t(log2(((t(r)+0.5)/(R+1))* 10^6))}
top_genes_index=function(g,X){return(order(rowSums(X),decreasing =TRUE)[1:g])}
Y=lcpm(r)
subset = top_genes_index(6,Y)
r = r[subset,]
r
```

### randomly sample cases and controls
In this example we use 2 cases vs 2 controls, and put labes on them by `condition`, based on which a design matrix is generated.

```{r}
counts = r[,sample(1:ncol(r),2*2)]
counts
condition = c(rep(0,2),rep(1,2))
condition
design = model.matrix(~condition)
design
```

### calculate normalizing factors for each sample
<span style="color:red">This might be potentially a problem, because the normalizing factors are only calculated based on top $g$ genes.</span>
1. A `DGEList` object is generated by `counts` and `condition`, using `edgeR::DGEList`
2. The normalizing factors are calculated by `edgeR::calcNormFactors`.

```{r}
DGEList_obj = edgeR::DGEList(counts=counts,group=condition)
DGEList_obj
dgecounts = edgeR::calcNormFactors(DGEList_obj)
dgecounts
```

### Calculate weight for each $\log_2$ expression using `limma::voom`
`limma::voom` takes in counts and normalizing factors, and gives, among other things, modified library sizes, $\log_2$ expression, weights for each expression, all for Gaussian-based linear modeling.

<span style="color:red">The peculiarity of `limma::voom` lies in the way it calculates the "average" log2-count for each gene.</span>: `sx <- fit$Amean + mean(log2(lib.size + 1)) - log2(1e+06)`.  Note that for each gene, the count as well as log2-count could vary wildly from sample to sample due to library size, sequencing depth, and / or experimental design, so the way to find an "average" is not self-evident.

```{r}
v = voom(dgecounts, design, plot=FALSE)
v
```

Here is what `limma::voom` does as in [Law et al 2013](https://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-2-r29)

voom: variance modeling at the observation-level

The limma-trend pipeline models the variance at the gene level. However, in RNA-seq applications, the count sizes may vary considerably from sample to sample for the same gene. Different samples may be sequenced to different depths, so different count sizes may be quite different even if the cpm values are the same. For this reason, we wish to model the mean-variance trend of the log-cpm values at the individual observation level, instead of applying a gene-level variability estimate to all observations from the same gene.

Our strategy is to estimate non-parametrically the mean-variance trend of the logged read counts and to use this mean-variance relationship to predict the variance of each log-cpm value. The predicted variance is then encapsulated as an inverse weight for the log-cpm value. When the weights are incorporated into a linear modeling procedure, the mean-variance relationship in the log-cpm values is effectively eliminated.
A technical difficulty is that we want to predict the variances of individual observations although there is, by definition, no replication at the observational level from which variances could be estimated. We work around this inconvenience by estimating the mean-variance trend at the gene level, then interpolating this trend to predict the variances of individual observations (Figure 2).
https://static-content.springer.com/image/art%3A10.1186%2Fgb-2014-15-2-r29/MediaObjects/13059_2013_Article_3227_Fig2_HTML.jpg
Figure 2

voom mean-variance modeling. (a) Gene-wise square-root residual standard deviations are plotted against average log-count. (b) A functional relation between gene-wise means and variances is given by a robust LOWESS fit to the points. (c) The mean-variance trend enables each observation to map to a square-root standard deviation value using its fitted value for log-count. LOWESS, locally weighted regression.

The algorithm proceeds as follows. First, gene-wise linear models are fitted to the normalized log-cpm values, taking into account the experimental design, treatment conditions, replicates and so on. This generates a residual standard deviation for each gene (Figure 2a). A robust trend is then fitted to the residual standard deviations as a function of the average log-count for each gene (Figure 2b).

Also available from the linear models is a fitted value for each log-cpm observation. Taking the library sizes into account, the fitted log-cpm for each observation is converted into a predicted count. The standard deviation trend is then interpolated to predict the standard deviation of each individual observation based on its predicted count size (Figure 2c). Finally, the inverse squared predicted standard deviation for each observation becomes the weight for that observation.

The log-cpm values and associated weights are then input into limmaâ€™s standard differential expression pipeline. Most limma functions are designed to accept quantitative weights, providing the ability to perform microarray-like analyses while taking account of the mean-variance relationship of the log-cpm values at the observation level.

### $t$-test using `limma::lmFit`

Taking in the `EList` from `limma::voom`, `limma::lmFit` produces $g$ $t$-tests, giving vectors of $\hatbeta$, $\hat s$, degree of freedom.

Note that `$stdev.unscaled`$= (X^TWX)^{-1}$, $W$ being the weights calculated by `limma::voom`.

```{r}
lim = lmFit(v)
lim
betahat.lim = lim$coefficients[,2]
betahat.lim
sebetahat.lim = lim$stdev.unscaled[,2]*lim$sigma
sebetahat.lim
t.lim = betahat.lim / sebetahat.lim
df.lim = lim$df.residual
df.lim
```

### empirical Bayes by `limma::eBayes`

<span style="color:red">Is it a variance shrinkage procedure?</span>

Taking in summary statistics ($\hat \beta$, $\hat s$, d.f.) for each gene, `limma::eBayes` produces shrunk standard deviation for each gene, and thus, moderated $t$-statistics and $p$-values.

```{r}
lim.ebayes = ebayes(lim)
lim.ebayes
sebetahat.ebayes = lim$stdev.unscaled[, 2] * sqrt(lim.ebayes$s2.post)
sebetahat.ebayes
t.ebayes = betahat.lim / sebetahat.ebayes
t.ebayes
lim.ebayes$t[, 2]
df.ebayes = lim.ebayes$df.total
df.ebayes
p.ebayes = (1 - pt(abs(t.ebayes), df.ebayes)) * 2
p.ebayes
lim.ebayes$p.value[, 2]
```

### obtain $z$ scores from moderated $p$-values and $t$-statistics

```{r}
z = qnorm(1 - lim.ebayes$p[, 2] / 2) * sign(lim.ebayes$t[, 2])
z
```


## Session Information

```{r session-info}
```
