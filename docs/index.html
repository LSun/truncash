<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Truncated Adaptive Shrinkage (truncash)</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">truncash</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/LSun/truncash">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Truncated Adaptive Shrinkage (<code>truncash</code>)</h1>

</div>


<p><code>truncash</code> (Truncated ASH) is an exploratory project with Matthew, built on <a href="https://github.com/stephens999/ashr"><code>ashr</code></a>.</p>
<ul>
<li><a href="voom_null.html">Matthew’s initial observation on null, correlated data</a></li>
</ul>
<p><a href="http://stephenslab.uchicago.edu/">Prof. Matthew Stephens</a> did a quick investigation of the p values and z scores obtained for simulated null data (using just voom transform, no correction) from real RNA-seq data of <a href="http://www.gtexportal.org/home/">GTEx</a>. Here is what he found.</p>
<p>“I found something that I hadn’t realized, although is obvious in hindsight: although you sometimes see inflation under null of <span class="math inline">\(p\)</span>-values/<span class="math inline">\(z\)</span>-scores, the most extreme values are not inflated compared with expectations (and tend to be deflated). That is the histograms of <span class="math inline">\(p\)</span>-values that show inflation near <span class="math inline">\(0\)</span> (and deflation near <span class="math inline">\(1\)</span>) actually hide something different going on in the very left hand side near <span class="math inline">\(0\)</span>. The qq-plots are clearer… showing most extreme values are deflated, or not inflated. This is expected under positive correlation i think. For example, if all <span class="math inline">\(z\)</span>-scores were the same (complete correlation), then most extreme of n would just be <span class="math inline">\(N(0,1)\)</span>. but if independent the most extreme of n would have longer tails…”</p>
<p>Matthew’s initial observation inspired this project. If under positive correlation, the most extreme tend to be not inflated, maybe we can use them to control the false discoveries. Meanwhile, if the moderate are more prone to inflation due to correlation, maybe it’s better to make only partial use of their information.</p>
<ul>
<li><a href="ExtremeOccurrence.html">Occurrence of extreme observations</a></li>
</ul>
<p>As <a href="https://galton.uchicago.edu/~stein/">Prof. Michael Stein</a> pointed during a conversation with Matthew, if the marginal distribution is correct then the expected number exceeding any threshold should be correct. So if the tail is “usually”&quot; deflated, it should be that with some small probability there are many large <span class="math inline">\(z\)</span>-scores (even in the tail). Therefore, if “on average” we have the right number of large <span class="math inline">\(z\)</span>-scores/small <span class="math inline">\(p\)</span>-values, and “usually” we have too few, then “rarely” we should have too many. A simulation is run to check this intuition.</p>
<ul>
<li><a href="StepDown.html">Two FWER-controlling procedures on correlated null</a></li>
</ul>
<p>In order to understand the behavior of <span class="math inline">\(p\)</span>-values of top expressed, correlated genes under the global null, simulated from GTEx data, we apply two FWER-controlling multiple comparison procedures, Holm’s “step-down” ([Holm 1979]) and Hochberg’s “step-up.” ([Hochberg 1988])</p>
<ul>
<li><p><a href="truncash.html"><code>truncash</code> Model and first simulations</a></p></li>
<li><p><a href="nullpipeline.html">Pipeline for simulating null data</a></p></li>
</ul>
<p>Using a toy model to examine and document the pipeline to simulate null summary statistics at each step, including <code>edgeR::calcNormFactors</code>, <code>limma::voom</code>, <code>limma::lmFit</code>, <code>limma::eBayes</code>.</p>
<ul>
<li><a href="FDR_Null.html">FDR on Null, Part 1</a></li>
<li><a href="FDR_null_betahat.html">FDR on Null, Part 2</a></li>
</ul>
<p>Apply two FDR-controlling procedures, BH and BY, as well as two <span class="math inline">\(s\)</span> value models, <code>ash</code> and <code>truncash</code> to the simulated, correlated null data, and compare the numbers of false discoveries (by definition, all discoveries should be false) obtained. Part 1 uses <span class="math inline">\(z\)</span> scores only, Part 2 uses <span class="math inline">\(\hat \beta\)</span> and moderated <span class="math inline">\(\hat s\)</span>.</p>
<ul>
<li><a href="pihat0_null.html"><span class="math inline">\(\hat\pi_0\)</span> estimated in correlated global null</a></li>
</ul>
<p><span class="math inline">\(\hat\pi_0\)</span> estimated by <code>ash</code> and <code>truncash</code> with <span class="math inline">\(T = 1.96\)</span> on correlated global null data simulated from GTEx/Liver. Ideally they should be close to <span class="math inline">\(1\)</span>.</p>
<ul>
<li><a href="cutoff_null.html">Ordered <span class="math inline">\(p\)</span> values vs critical values</a></li>
</ul>
<p>For various FWER / FDR controlling procedures, and for <code>truncash</code>, what matters the most is the behavior of the most extreme observations. Here these extreme <span class="math inline">\(p\)</span> values are plotted along with common critical values used by various procedures, in order to shed light on their behavior.</p>
<p>It’s very exploratory. May be related to Extreme Value Thoery and Concentration of Measure. To be continued.</p>
<ul>
<li><a href="SingleExtOb.html">Single most extreme observation</a></li>
</ul>
<p>What will happen if we allow the threshold in <code>truncash</code> dependent on data? Let’s start from the case when we only know the single most extreme observation.</p>
<ul>
<li><a href="t-likelihood.html">Handling <span class="math inline">\(t\)</span> likelihood</a></li>
</ul>
<p>When moving to <span class="math inline">\(t\)</span> likelihood, or in other words, when taking randomness of <span class="math inline">\(\hat s\)</span> into consideration, things get trickier. Here we review several techiniques currently used in Matthew’s lab, regarding <span class="math inline">\(t\)</span> likelihood and uniform mixture priors, based on a discussion with Matthew.</p>
<ul>
<li><a href="correlated_z.html">Histogram of correlated <span class="math inline">\(z\)</span> scores, random data sets</a></li>
<li><a href="correlated_z_2.html">Histogram of correlated <span class="math inline">\(z\)</span> scores, <code>ash</code>-hostile data sets</a></li>
<li><a href="correlated_z_3.html">Histogram of correlated <span class="math inline">\(z\)</span> scores, <code>BH</code>-hostile data sets</a></li>
</ul>
<p>An implicit key question of this inquiry is: what’s the behavior of <span class="math inline">\(z\)</span> scores under dependency? We take a look at their histograms. First for randomly sampled data sets. Second for those most “hostile” to <code>ash</code>. Third for those most “hostile” to <code>BH</code>. The bottom line is we are reproducing what Efron observed in microarray data, that “the theoretical null may fail” in different ways. Now the key questions are</p>
<ol style="list-style-type: decimal">
<li><p>Why the theoretical null may fail? What does it mean by <strong>correlation</strong>?</p></li>
<li><p>Can <code>truncash</code> make <code>ash</code> more robust against some of the foes that make the theoretical null fail?</p></li>
<li><p>Generally, how robust is empirical Bayes? Is empirical Bayes robust or non-robust to certain kinds of correlation?</p></li>
</ol>
<ul>
<li><a href="gaussian_derivatives.html">Fitting empirical null with Gaussian derivatives: Theory</a></li>
<li><a href="gaussian_derivatives_2.html">Fitting empirical null with Gaussian derivatives: Examples</a></li>
<li><a href="gaussian_derivatives_3.html">Fitting empirical null with Gaussian derivatives: Numerical issues</a></li>
<li><a href="gaussian_derivatives_4.html">Fitting empirical null with Gaussian derivatives: Large correlations</a></li>
<li><a href="gd_delta.html">Fitting empirical null with Gaussian derivatives: Delta function</a></li>
</ul>
<p>Inspired by <a href="http://amstat.tandfonline.com/doi/abs/10.1198/jasa.2010.tm10237">Schwartzman 2010</a>, we experiment a new way to tackle “empirical null.”</p>
<ul>
<li><a href="gaussian_derivatives_5.html">Fitting empirical null with Gaussian derivatives: Weight constraints</a></li>
</ul>
<p>In Gaussian derivatives decomposition, weights <span class="math inline">\(W_k\)</span> and especially <a href="gaussian_derivatives_5.html#numerical_issues">normalized weights</a> <span class="math inline">\(W_k^s = W_k\sqrt{k!}\)</span> contain substantial information. In order to produce a proper density, and in order to regularize the fitted density to make it describe a plausible empiricall correlated null, constraints need to be imposed on the weights.</p>
<ul>
<li><a href="alternative.html">True signal vs correlated null: small effects</a></li>
<li><a href="alternative2.html">True signal vs correlated null: larger effects</a></li>
</ul>
<p>Both true effects and correlation can distort the empirical distribution away from the standard normal <span class="math inline">\(N(0, 1)\)</span>, and Gaussian derivatives are presumably able to fit both. Therefore, is there a way to let Gaussian derivatives with <a href="gaussian_derivatives_5.html#weight_constraints">a reasonable number of reasonable weights</a> automatically identify correlated null from true effects? It works well when effects are <em>not too small</em> right now.</p>
<ul>
<li><a href="fitting_normal.html">Fitting <span class="math inline">\(N\left(0, \sigma^2\right)\)</span> with Gaussian derivatives</a></li>
</ul>
<p>Continuing from the empirical studies above, we are looking at why <span class="math inline">\(N\left(0, \sigma^2\right)\)</span> when <span class="math inline">\(\sigma^2\)</span> is small can be satisfactorily fitted by a limited number of Gaussian derivatives.</p>
<ul>
<li><a href="diagnostic_plot.html">Diagnostic plots for <code>ASH</code></a></li>
<li><a href="diagnostic_correlated_z.html">Diagnostic plots for <code>ASH</code> on correlated null</a></li>
<li><a href="diagnostic_correlated_z_2.html">Diagnostic plots for <code>ASH</code> on correlated null: <code>BH</code> vs <span class="math inline">\(\hat\pi_0\)</span></a></li>
<li><a href="diagnostic_correlated_z_3.html">Diagnostic plots for <code>ASH</code> on correlated null: <code>BH</code> vs <code>lfsr</code></a></li>
</ul>
<p>Under the exchangeability assumption, the goodness of fit of empirical Bayes can be measured by the behavior of <span class="math inline">\(\left\{\hat F_j = \hat F_{\hat\beta_j | \hat s_j}\left(\hat\beta_j\mid\hat s_j\right)\right\}\)</span>. Meanwhile, if <code>ASH</code> is applied to correlated null <span class="math inline">\(z\)</span> scores, estimated <span class="math inline">\(\hat g\)</span> will not only be different from <span class="math inline">\(\delta_0\)</span>; moreover, with this estimated <span class="math inline">\(\hat g\)</span>, <span class="math inline">\(\left\{\hat F_j\right\}\)</span> might not behave like <span class="math inline">\(\text{Unif}\left[0, 1\right]\)</span>.</p>
<ul>
<li><a href="diagnostic_plot_2.html">Diagnostic plots for uniform distribution</a></li>
<li><a href="diagnostic_ash.html">Diagnostic plots for <code>ASH</code></a></li>
<li><a href="plot_diagnostic.html"><code>ashr::plot_diagnostic()</code> in <code>ashr</code></a></li>
</ul>
<p>Essentially we hope to tell whether <span class="math inline">\(n\)</span> random samples are uniformly distributed, and the task seems more complicated than what it sounds. There are multiple ways to do it, and some of them have been absorbed into the <code>ashr</code> package.</p>
<ul>
<li><a href="marginal_z.html">Marginal distribution of <span class="math inline">\(z\)</span> scores: Correlated null</a></li>
<li><a href="marginal_z_alternative.html">Marginal distribution of <span class="math inline">\(z\)</span> scores: Correlated non-null</a></li>
</ul>
<p>Take a look at the null <span class="math inline">\(z\)</span> scores we’ve been experimenting with so far and check whether they truly are marginally <span class="math inline">\(N\left(0, 1\right)\)</span>. It’s an enhanced version of the simulation on <a href="ExtremeOccurrence.html">occurrence of extreme observations</a>. The non-null <span class="math inline">\(z\)</span> scores are produced for comparison.</p>
<ul>
<li><a href="create_null.html">Can we create correlated null from the raw data?</a></li>
</ul>
<p>This whole project comes from a ubiquitous issue that in the simultaneous inference problem, the deviation from what would be expected under the null can come from both distortion due to correlation and enrichment due to true effects. Ideally, it would be perfect if we could create controls which keep the distortion by correlation unchanged, but remove the true effects. Efron in a series of papers (for example, <a href="http://www.tandfonline.com/doi/abs/10.1198/016214501753382129">Efron et al 2001</a>) suggests to create the null from the raw data by resampling or permutation. We apply the idea to our data sets, and the results are not promising.</p>
<ul>
<li><a href="ash_gd.html">Normal means inference with heteroskedastic and correlated noise: Model</a></li>
</ul>
<p>The previous investigation is boiled down to a prototypical problem: the classic normal means inference, but with heteroskedastic and especially correlated noise. Gaussian derivatives and <code>ASH</code> provide a way to tackle it.</p>
<ul>
<li><a href="Rmosek.html">Implementation by <code>Rmosek</code>: Correlated null</a></li>
<li><a href="gd-ash.html">Implementation by <code>Rmosek</code>: Simulated non-null</a></li>
</ul>
<p><code>cvxr</code> is still under active development. We are moving to the more established <code>Rmosek</code>, exploring and experimenting this convex optimization package.</p>
<ul>
<li><a href="mosek_reg.html">Improvement on implementation with <code>Rmosek</code>: Primal and dual</a></li>
<li><a href="mosek_reg_2.html">Improvement on implementation with <code>Rmosek</code>: Normalization</a></li>
<li><a href="mosek_reg_3.html">Improvement on implementation with <code>Rmosek</code>: Regularization</a></li>
</ul>
<p>Several ways are explored to make fitting Gaussian derivatives more efficient and accurate.</p>
<ul>
<li><a href="mosek_reg_4.html">Regularized Gaussian Derivatives: Correlated null</a></li>
<li><a href="mosek_reg_5.html">Regularized Gaussian Derivatives &amp; <code>ASH</code>: Simulated true effects with homoscedastic correlated noises</a></li>
<li><a href="mosek_reg_6.html">Regularized Gaussian Derivatives &amp; <code>ASH</code>: Simulated true effects with heteroskedastic correlated noises</a></li>
</ul>
<p>Fitting the corrrelated null with Gaussian derivatives has two constraints: non-negativity and orderly decay, and both are intractable. We are using <span class="math inline">\(L_1\)</span> regularization to try to solve both.</p>
<ul>
<li><a href="real_data_simulation.html">Simulated signals added to real data: Part 1</a></li>
<li><a href="real_data_simulation_2.html">Simulated signals added to real data: Part 2</a></li>
<li><a href="real_data_simulation_3.html">Simulated signals added to real data: Part 3</a></li>
<li><a href="real_data_simulation_4.html">Simulated signals added to real data: Part 4</a></li>
<li><a href="real_data_simulation_5.html">Simulated signals added to real data: Part 5</a></li>
</ul>
<p>Using the <code>seqgendiff</code> pipeline developed by <a href="http://dcgerard.github.io/">David</a>, <a href="https://github.com/mengyin">Mengyin</a>, and Matthew, we are adding simulated <span class="math inline">\(\pi_0\delta_0 + \left(1 - \pi0\right)N\left(0, \sigma^2\right)\)</span> signals to GTEx/Liver RNA-seq expression matrix, and compare our method with <a href="http://www.jstor.org/stable/2346101"><code>BH</code></a>, <a href="http://bioconductor.org/packages/release/bioc/html/qvalue.html"><code>qvalue</code></a>, <a href="https://cran.r-project.org/web/packages/locfdr/index.html"><code>locfdr</code></a>, <a href="https://github.com/stephens999/ashr"><code>ash</code></a>, <a href="https://www.bioconductor.org/packages/release/bioc/html/sva.html"><code>sva</code></a>, <a href="https://cran.r-project.org/web/packages/cate/index.html"><code>cate</code></a>, <a href="https://github.com/dcgerard/vicar"><code>mouthwash</code></a>. The pipeline and result here are mostly exploratory and in development stage. More mature version will follow.</p>
<ul>
<li><a href="simulated_correlated_null.html">Completely synthetic correlated null: Low rank simulation</a></li>
<li><a href="simulated_correlated_null_2.html">Completely synthetic correlated null: Multiple groups of closely correlated null</a></li>
<li><a href="simulated_correlated_null_3.html">Completely synthetic correlated null: More low rank simulation</a></li>
<li><a href="simulated_correlated_null_4.html">Completely synthetic correlated null: Limit case of low rank simulation</a></li>
</ul>
<p>Following <a href="http://home.uchicago.edu/gaow/">Gao</a>’s suggestion, we are taking a look at whether Gaussian derivatives can satisfactorily fit synthetic correlated <span class="math inline">\(N\left(0, 1\right)\)</span> <span class="math inline">\(z\)</span> scores. Suggested by Matthew, <strong>it might have something to do with the distribution of eigenvalues in a random matrix</strong>, and the <a href="https://en.wikipedia.org/wiki/Wigner_semicircle_distribution">Wigner semicircle distribution</a>. It’s also an interesting comparison with Gaussian mixtures.</p>
<ul>
<li><a href="simulation_real_se.html">Simulated true effects with realistic correlated noises: Large and moderate SNR</a></li>
<li><a href="simulation_real_se_2.html">Simulated true effects with realistic correlated noises: Small SNR</a></li>
</ul>
<p>The large-scale simulation testing for GD-ASH with simulated effects under different sparsity, strength, and shape, and realistic correlated noises.</p>
<ul>
<li><a href="auc_pvalue.html">Why AUC of <span class="math inline">\(p\)</span> values and <code>BH</code> might be different?</a></li>
</ul>
<p>AUC-wise, <span class="math inline">\(p\)</span> values and <span class="math inline">\(BH\)</span> should perform almost the same, since <code>BH</code> doesn’t change the order of <span class="math inline">\(p\)</span> values, but in practice their performance might differ. It turns out ties should be one important reason.</p>
<ul>
<li><a href="group_meeting_0608.html">Group meeting presentation on <code>GD-ASH</code></a></li>
</ul>
<p>Talked about using Gaussian derivatives to handle correlation at Matthew’s group meeting.</p>
<ul>
<li><a href="smemo.html"><code>GD-ASH</code> applied to Smemo et al 2014 data</a></li>
<li><a href="smemo_2.html"><code>GD-ASH</code> applied to Smemo et al 2014 data: Fitting <code>ASH</code> first</a></li>
</ul>
<p>An RNA-seq data set of mouse hearts contains <span class="math inline">\(23K\)</span>+ genes and only 4 samples, two for each condition. <code>GD-ASH</code> applying to this data set raises several interesting questions.</p>
<ul>
<li><a href="gd_lik.html"><code>GD-ASH</code> with Gaussian derivative likelihood: Initial simulation</a></li>
<li><a href="gd_lik_2.html"><code>GD-ASH</code> with Gaussian derivative likelihood: Model, formulae, and simulation</a></li>
</ul>
<p>When the noise is correlated, the likelihood should be correlated. However, in the <a href="ash_gd.html"><code>GD-ASH</code></a> model, noise is essentially treated to be independently sampled from an empirical null fitted by Gaussian derivatives, so it would make more sense to use Gaussian derivatives, rather than simple Gaussian, as likelihood.</p>
<ul>
<li><a href="rmosek_primal_dual.html">Primal vs Dual in <code>Rmosek</code></a></li>
<li><a href="rmosek_primal_dual_2.html">Primal vs Dual in <code>Rmosek</code>: With or without <span class="math inline">\(L_1\)</span> regularization</a></li>
</ul>
<p>A more in-depth look into the <a href="mosek_reg.html">primal vs dual</a> problem in <code>Rmosek</code> implementation.</p>
<ul>
<li><a href="brca.html">Data analysis: Efron’s BRCA data</a></li>
</ul>
<p>A close look at Efron’s classic BRCA data, used in many of his papers to illustrate the empirical Bayes idea which inspires this project.</p>
<ul>
<li><a href="cash_sim_1.html"><code>CASH</code> Simulation, Part 1: Overall data sets</a></li>
<li><a href="cash_sim_2.html"><code>CASH</code> Simulation, Part 2: Over-dispersed data sets</a></li>
<li><a href="cash_sim_3.html"><code>CASH</code> Simulation, Part 3: Under-dispersed data sets</a></li>
<li><a href="cash_sim_4.html"><code>CASH</code> Simulation, Part 4: Different effect size distributions</a></li>
<li><a href="cash_sim_5.html"><code>CASH</code> Simulation, Part 5: pFDP variability</a></li>
<li><a href="cash_sim_6.html"><code>CASH</code> Simulation, Part 6: Synthetic vs real data</a></li>
<li><a href="cash_sim_7.html"><code>CASH</code> Simulation, Part 7: More on synthetic data</a></li>
</ul>
<p>Using the same framework of <a href="gd_lik_2.html">Gaussian derivative likelihood</a>, extensive simulations are performed to compare <code>CASH</code> with other methods, smaller simulations are used to facilitate the exploration, different visualization methods are explored to illustrate the advantages of <code>CASH</code>.</p>
<ul>
<li><a href="cash_fdr_1.html"><code>CASH</code> on FDR, Part 1: <span class="math inline">\(g = 0.5\delta_0 + 0.5N(0, \sigma_n^2)\)</span></a></li>
<li><a href="cash_fdr_2.html"><code>CASH</code> on FDR, Part 2: <span class="math inline">\(g = 0.5\delta_0 + 0.5N(0, (2\sigma_n)^2)\)</span></a></li>
<li><a href="cash_fdr_3.html"><code>CASH</code> on FDR, Part 3: <span class="math inline">\(g = 0.9\delta_0 + 0.1N(0, \sigma_n^2)\)</span></a></li>
<li><a href="cash_fdr_4.html"><code>CASH</code> on FDR, Part 4: <span class="math inline">\(g = 0.9\delta_0 + 0.1N(0, (2\sigma_n)^2)\)</span></a></li>
<li><a href="cash_fdr_5.html"><code>CASH</code> on FDR, Part 5: <span class="math inline">\(g = 0.5\delta_0 + 0.25N(-2\sigma_n, \sigma_n^2) + 0.25N(2\sigma_n, \sigma_n^2)\)</span></a></li>
</ul>
<p>An extensive study of <code>CASH</code> and other methods in different scenarios shows that</p>
<ol style="list-style-type: decimal">
<li><p><code>CASH</code> produces more accurate <span class="math inline">\(\hat\pi_0\)</span>;</p></li>
<li><p><code>CASH</code> produces less variable false discovery proportions at nominal FDR cutoff;</p></li>
<li><p><code>CASH</code> tends to overfit when the noise is under-dispersed or looks like independent;</p></li>
<li><p><code>CASH</code> doesn’t work well when <em>Unimodal Assumption (UA)</em> is broken.</p></li>
</ol>
<ul>
<li><a href="cash_deconv.html"><code>CASH</code> on Deconvolution: Accuracy of <span class="math inline">\(\hat g\)</span></a></li>
</ul>
<p>On the deconvolution front, <code>CASH</code> generates more robust <span class="math inline">\(\hat g\)</span> than <code>ASH</code> in the correlated noise case, and doesn’t show noticeable overfitting in the independent noise case.</p>
<!-- The goal of this new template is to simplify the setup and maintenance of a research website. -->
<!-- Specifically, -->
<!-- *  Easier to build and extend the website using the new tools in the [rmarkdown][] package and [latest RStudio release][rstudio] -->
<!-- *  Easier to deploy the website with Git and GitHub by only using one branch -->
<!-- [rmarkdown]: http://rmarkdown.rstudio.com/rmarkdown_websites.htm -->
<!-- [rstudio]: https://www.rstudio.com/products/rstudio/download/preview/ -->

<hr>
<p>
    This <a href="http://rmarkdown.rstudio.com">R Markdown</a> site was created with <a href="https://github.com/jdblischak/workflowr">workflowr</a>
</p>
<hr>

<!-- To enable disqus, uncomment the section below and provide your disqus_shortname -->

<!-- disqus
  <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'rmarkdown'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
